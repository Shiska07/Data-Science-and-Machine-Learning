{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from file and create vectors\n",
    "The function **get_vectors** takes a filename as input and returns a matrix X containing each row was a datapoint and vector y containing target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(filename):\n",
    "    try:\n",
    "        f = open(filename, 'r')\n",
    "    except OSError:\n",
    "        print(f'{filename} could not be opened.\\n')\n",
    "        sys.exit()\n",
    "        \n",
    "    # initialize list to store feature and labels for training data\n",
    "    features = []             \n",
    "    labels = []\n",
    "    \n",
    "    with f:\n",
    "        line = f.readline()\n",
    "        while line != '':\n",
    "            # strip newline and outer parenthesis\n",
    "            line = line.strip('\\n')\n",
    "            line = line.strip('( )')\n",
    "            \n",
    "            # extrace label and append to labels list\n",
    "            single_label = line.split('), ')[-1]\n",
    "            labels.append(single_label)\n",
    "            \n",
    "            # extrace features and append to features list\n",
    "            feat = line.split('), ')[0].split(', ')\n",
    "            features.append(feat)\n",
    "            \n",
    "            # read next line\n",
    "            line = f.readline()\n",
    "        \n",
    "        # create dataframe of features and append labels\n",
    "        X = np.array(features, dtype = float, ndmin = 2)\n",
    "        \n",
    "        # convert labels list to array\n",
    "        y = np.array(labels, dtype = str, ndmin = 2)\n",
    "        \n",
    "        return X, y.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates euclidean distance between training datapoints and test data point\n",
    "def get_cartesian_distance(X_train, p):\n",
    "    \n",
    "    # n = total number of datapoints, f_n = total number of features\n",
    "    n, f_n = X_train.shape\n",
    "    \n",
    "    sum_of_squared_diff = np.zeros((n, 1), dtype = float)\n",
    "    \n",
    "    # use vectorization to get sum of squared difference\n",
    "    for i in range(f_n):\n",
    "        x_vector = X_train[:,i].reshape((n,1))\n",
    "        sum_of_squared_diff = sum_of_squared_diff + (x_vector - p[i])**2\n",
    "        \n",
    "    # take sq root to get array of cartesianeuclidean distance\n",
    "    euc_dist = np.sqrt(sum_of_squared_diff)\n",
    "    \n",
    "    return euc_dist\n",
    "\n",
    "# This function calculates manhattan distance between training datapoints and test data point\n",
    "def get_manhattan_distance(X_train, p):\n",
    "    \n",
    "    # n = total number of datapoints, f_n = total number of features\n",
    "    n, f_n = X_train.shape\n",
    "    \n",
    "    sum_of_abs_diff = np.zeros((n, 1), dtype = float)\n",
    "    \n",
    "    # use vectorization to get sum of squared difference\n",
    "    for i in range(f_n):\n",
    "        x_vector = X_train[:,i].reshape((n,1))\n",
    "        sum_of_abs_diff = sum_of_abs_diff + abs(x_vector - p[i])\n",
    "        \n",
    "    # take sq root to get array of cartesianeuclidean distance\n",
    "    man_dist = np.sqrt(sum_of_abs_diff)\n",
    "    \n",
    "    return man_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Implementation \n",
    "\n",
    "The **predict_class_label** function takes training data(X_train, y_train), prediction datapoint(p), number of neighbors(k) to consider and distance type for similarity measurement(dist_type) as arguments and returns a prediction class based on highest posterior class probability value i.e. argmax_c P(class|< data >).\n",
    "\n",
    "Key notes:\n",
    "1. The distance, prior_class_probabilities and labels of the first 'k' neighbors are printed if verbose = 1.\n",
    "2. Tie breaker: If two or more classes have the same highest probability value, class label of the closest neighbor among them\n",
    "   is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_class_with_knn(X_train, y_train, p, k, dist_type, verbose = 'n'):\n",
    "    \n",
    "    # n = total number of datapoints, f_n = total number of features\n",
    "    n, f_n = X_train.shape\n",
    "    \n",
    "    if dist_type == 'cartesian':\n",
    "        dist_arr = get_cartesian_distance(X_train, p)\n",
    "    elif dist_type == 'manhattan':\n",
    "        dist_arr = get_manhattan_distance(X_train, p)\n",
    "    \n",
    "    # concat with y_train labels and sort in ascending order of the distance\n",
    "    dist_arr = np.concatenate((dist_arr, y_train), axis = 1)\n",
    "    dist_arr = dist_arr[dist_arr[:,0].argsort()]\n",
    "    \n",
    "    # the first 'k' rows contain distance and labels of the k nearest neighbors\n",
    "    knn = dist_arr[0:k,:]\n",
    "    \n",
    "    # save class labels of the k nearest neighbors as a list s.t. to count occurence\n",
    "    knn_labels = list(knn[:,1])\n",
    "    \n",
    "    # class labels\n",
    "    class_labels = list(set(knn_labels))\n",
    "    \n",
    "    # calculate posterior class probability\n",
    "    class_probabilies = {}\n",
    "    class_probabilies['Metal'] = knn_labels.count('Metal')/n\n",
    "    class_probabilies['Ceramic'] = knn_labels.count('Ceramic')/n\n",
    "    class_probabilies['Plastic'] = knn_labels.count('Plastic')/n\n",
    "   \n",
    "    # knn_post_class_prob stores posterior probability of the 'k' nearest neighbors based on their class label\n",
    "    knn_post_class_prob = []\n",
    "    \n",
    "    for idx, item in enumerate(knn_labels):\n",
    "        knn_post_class_prob.append(class_probabilies[item]) \n",
    "    \n",
    "    max_class_prob_idx = 0\n",
    "    max_class_prob = float(0)\n",
    "    \n",
    "    for i in range(k):\n",
    "        if knn_post_class_prob[i] > max_class_prob:\n",
    "            \n",
    "            # value of highest posterior class probability\n",
    "            max_class_prob = knn_post_class_prob[i]\n",
    "            \n",
    "            # index of neighbor with highest posterior class probability\n",
    "            max_class_prob_idx = i\n",
    "            \n",
    "    if verbose == 'y':\n",
    "        print(f'\\nFor datapoint {p} the {k} nearest neighbors are:')\n",
    "        print(knn)\n",
    "        print(f'The posterior class probabilities are:')\n",
    "        for key, val in class_probabilies.items():\n",
    "            print(f'{key}: {val:0.4f}')\n",
    "        \n",
    "    # return the class with maximum MLP and the probablity value\n",
    "    return knn_labels[max_class_prob_idx], knn_post_class_prob[max_class_prob_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out evaluation function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **leave_one_out_evaluation** function takes the entire feature dataset(X) , correct data point labels(y), the number of neighbors to consider(k), verbose preference and the type of distance to use for nearest neighbor calculation(dist_type) as arguments and returns the accuracy (total correct predictions/ total datapoints). \n",
    "\n",
    "The datapoint ot be left out and tested is chosen according to its index (i.e. item at index 0 is left out during the first iteration and item at index n-1 is left our during the last iteration).\n",
    "\n",
    "The **get_evaluation_results** function takes the entire feature dataset(X) , correct data point labels(y)and the type of distance to use for nearest neighbor calculation(dist_type) and verbose preference as arguments and calls **leave_one_out_evaluation** function to get accuracy values for different values of k(1, 3 and 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out_evaluation(X, y, k, dist_type, verbose):\n",
    "    \n",
    "    # get number of training items and number of features\n",
    "    n, f_n = X.shape\n",
    "    \n",
    "    # prediction labels generated by 'predict_class_with_knn' will be stored in this list\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        X_train = np.delete(X, i, axis = 0)\n",
    "        y_train = np.delete(y, i, axis = 0)\n",
    "        X_test = X[i,:]\n",
    "        pred, prob = predict_class_with_knn(X_train, y_train, X_test, k, dist_type, verbose)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # convert prediction list to numpy array\n",
    "    predictions = np.array(predictions, dtype = str, ndmin = 2)\n",
    "    predictions = predictions.reshape(predictions.shape[1], 1)\n",
    "    \n",
    "    # return accuracy\n",
    "    return (np.sum(y == predictions))/n\n",
    "\n",
    "\n",
    "def get_evaluation_results(X, y, dist_type, verbose):\n",
    "    # initialize distionary to store accuracy values for different 'k' values\n",
    "    accuracy = {}\n",
    "\n",
    "    # calculate accuracy for various values of 'k'\n",
    "    accuracy[1] = leave_one_out_evaluation(X, y, 1, dist_type, verbose)\n",
    "    accuracy[3] = leave_one_out_evaluation(X, y, 3, dist_type, verbose)\n",
    "    accuracy[5] = leave_one_out_evaluation(X, y, 5, dist_type, verbose)\n",
    "\n",
    "    print(f'With {dist_type} distance:')\n",
    "    for key, value in accuracy.items():\n",
    "        print(f'For k = {key} the accuracy is {value:0.6f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide training filename & prediction data point\n",
    "The file must contain 1 datapoint per line in format (( height, diameter, weight, hue ), label ) which is similar to the format provided for the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter file containing training data: Data\\\\2_a_train.txt\n",
      "Enter file containing test data: Data\\\\2_a_test.txt\n",
      "Enter value of k: 3\n",
      "\n",
      "\n",
      "Would you like to print detailed results?(y/n): y\n",
      "\n",
      "\n",
      "For datapoint [0.12671048 0.06804045 0.20859883 3.95879103] the 3 nearest neighbors are:\n",
      "[['0.22991008654808265' 'Ceramic']\n",
      " ['0.40132508774163955' 'Metal']\n",
      " ['0.46414161830789835' 'Plastic']]\n",
      "The posterior class probabilities are: {'Metal': 0.08333333333333333, 'Ceramic': 0.08333333333333333, 'Plastic': 0.08333333333333333}\n",
      "Datapoint 1 : 'Ceramic' with a probability of 0.0833.\n",
      "For datapoint [0.08007783 0.05160717 0.17838065 2.92909724] the 3 nearest neighbors are:\n",
      "[['0.1626000364448459' 'Ceramic']\n",
      " ['0.22205364725787488' 'Metal']\n",
      " ['0.32726564816026826' 'Ceramic']]\n",
      "The posterior class probabilities are: {'Metal': 0.08333333333333333, 'Ceramic': 0.16666666666666666, 'Plastic': 0.0}\n",
      "Datapoint 2 : 'Ceramic' with a probability of 0.1667.\n",
      "For datapoint [0.10538602 0.12242649 0.74997602 5.63607809] the 3 nearest neighbors are:\n",
      "[['0.9044107855365048' 'Plastic']\n",
      " ['0.927363218751139' 'Plastic']\n",
      " ['1.546894043294699' 'Ceramic']]\n",
      "The posterior class probabilities are: {'Metal': 0.0, 'Ceramic': 0.08333333333333333, 'Plastic': 0.16666666666666666}\n",
      "Datapoint 3 : 'Plastic' with a probability of 0.1667.\n",
      "For datapoint [0.19908879 0.14817769 0.67289149 4.08330651] the 3 nearest neighbors are:\n",
      "[['0.46655417686176764' 'Ceramic']\n",
      " ['0.4804424984691606' 'Metal']\n",
      " ['0.5563589821119621' 'Plastic']]\n",
      "The posterior class probabilities are: {'Metal': 0.08333333333333333, 'Ceramic': 0.08333333333333333, 'Plastic': 0.08333333333333333}\n",
      "Datapoint 4 : 'Ceramic' with a probability of 0.0833.\n"
     ]
    }
   ],
   "source": [
    "# provide training and test filename\n",
    "fname_train = str(input('Enter file containing training data: '))\n",
    "fname_test = str(input('Enter file containing test data: '))\n",
    "\n",
    "# provide 'k' value\n",
    "k = int(input('Enter value of k: '))\n",
    "print('\\n')\n",
    "\n",
    "# provide verbose preference\n",
    "verbose = str(input('Would you like to print detailed results?(y/n): '))\n",
    "print('\\n')\n",
    "\n",
    "# get training data as vectors\n",
    "X, y = get_vectors(fname_train)\n",
    "\n",
    "# read test file to make predictions\n",
    "with open(fname_test, 'r') as f:\n",
    "    line = f.readline()\n",
    "    i = 1\n",
    "    while line != '':\n",
    "        line = line.strip('\\n')\n",
    "        line = line.strip('( )')\n",
    "        values = line.split(', ')\n",
    "        p = np.array(values, dtype = float)\n",
    "        preidicted_class, prob_value = predict_class_with_knn(X, y, p, k, 'cartesian', verbose)\n",
    "        print(f\"Class prediction for datapoint {i} : '{preidicted_class}' with a probability of {prob_value:.4f}.\")\n",
    "        i = i + 1\n",
    "        line = f.readline()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter filename for leave-one-out evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter filename for leave-one-out evaluation data: Data\\\\2_c_d_e.txt\n"
     ]
    }
   ],
   "source": [
    "fname_loo = str(input('Enter filename for leave-one-out evaluation data: '))\n",
    "\n",
    "# provide verbose preference\n",
    "verbose = str(input('Would you like to print detailed results?(y/n): '))\n",
    "print('\\n')\n",
    "\n",
    "X, y = get_vectors(fname_loo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 1, 3 and 5 using cartesian distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With cartesian distance:\n",
      "For k = 1 the accuracy is 0.425000.\n",
      "For k = 3 the accuracy is 0.408333.\n",
      "For k = 5 the accuracy is 0.400000.\n"
     ]
    }
   ],
   "source": [
    "get_evaluation_results(X, y, 'cartesian', verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: k = 1 was found to give the best performance in terms of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 d) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K = 1, 3 and 5 using manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With manhattan distance:\n",
      "For k = 1 the accuracy is 0.500000.\n",
      "For k = 3 the accuracy is 0.458333.\n",
      "For k = 5 the accuracy is 0.516667.\n"
     ]
    }
   ],
   "source": [
    "get_evaluation_results(X, y, 'manhattan', verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Manhattan distance was found to perform better than cartesian distance. Accuracy is highest for a value of k = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction after removing 4th attribute (hue) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 4th attribute from X\n",
    "X_3 = np.delete(X, 3, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of x\n",
    "X_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With cartesian distance:\n",
      "For k = 1 the accuracy is 0.816667.\n",
      "For k = 3 the accuracy is 0.833333.\n",
      "For k = 5 the accuracy is 0.808333.\n"
     ]
    }
   ],
   "source": [
    "get_evaluation_results(X_3, y, 'cartesian', verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Removing the 4th attribute was found to significantly improve accuracy with highest accuracy value for k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
